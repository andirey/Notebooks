{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import random\n",
    "import math\n",
    "\n",
    "# from Chapter 4\n",
    "def squared_distance(v, w):\n",
    "    \"\"\"(v_1 - w_1) ** 2 + ... _ (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "\n",
    "def vector_subtract(v, w):\n",
    "    \"\"\"adds corresponding elemetns\"\"\"\n",
    "    return [v_i - w_i for v_i, w_i in zip(v, w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)\n",
    "\n",
    "print(sum_of_squares([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.001999999998844\n"
     ]
    }
   ],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "# test it?\n",
    "print(difference_quotient(lambda x: 2 * x ** 2 + x + 5, 2, 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x105658160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In python 3, map needs to be changed to list\n",
    "# also, partial needs to be imported from functools\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x\n",
    "\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.00001)\n",
    "\n",
    "# plot to show they're basically the same\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(-10,10)\n",
    "plt.title(\"Actualy Derivatives vs. Estimates\")\n",
    "plt.plot(x, list(map(derivative, x)), 'rx', label='Actual')  # red x\n",
    "plt.plot(x, list(map(derivative_estimate, x)), 'b+', label='Estimate') # blue +\n",
    "plt.legend(loc=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.00001000001393, 4.000010000027032]\n",
      "[4.000020000205495, 5.000000000165983]\n"
     ]
    }
   ],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0)   # add h to just the ith element of v\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]\n",
    "    \n",
    "print(estimate_gradient(sum_of_squares, [1, 2], h=0.00001))\n",
    "print(estimate_gradient(lambda v:  2 * v[0] ** 2 + 5 * v[1] + 5, [1, 2], h=0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.2129144872673417e-06, -3.2129144872673417e-06, 2.0080715545420907e-06]\n"
     ]
    }
   ],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True: \n",
    "    gradient = sum_of_squares_gradient(v)   # compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01)       # take a negative gradient step\n",
    "    \n",
    "    if distance(next_v, v) < tolerance:\n",
    "        break\n",
    "    v = next_v                              # continue if we/re not\n",
    "    \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing the Right Step Size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum v [-0.0006646139978924582, 0.0004984604984193437, -0.001163074496311801]\n",
      "minimum value 2.0429169186500064e-06\n"
     ]
    }
   ],
   "source": [
    "# step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "# defined again before\n",
    "\n",
    "def safe(f):\n",
    "    \"\"\"return a new function that's the same as f,\n",
    "    except that it outputs infinity whenever f produces an error\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')       # this means \"infiinity\" in Python\n",
    "    return safe_f\n",
    "\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient decent to find theta that minimizes target function\"\"\"\n",
    "\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0                    # set theta to initial value\n",
    "    target_fn = safe(target_fn)        # safe version of target_fn\n",
    "    value = target_fn(theta)           # value we're minimizing\n",
    "\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "        \n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "            \n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                         negate_all(gradient_fn),\n",
    "                         theta_0,\n",
    "                         tolerance)\n",
    "\n",
    "\n",
    "### Test ###\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "v = minimize_batch(sum_of_squares, sum_of_squares_gradient, v)\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))\n",
    "\n",
    "## from https://github.com/joelgrus/data-science-from-scratch/blob/master/code/gradient_descent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum_of_squares() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-3712f2e25769>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminimize_stochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_of_squares\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_of_squares_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minimum v\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"minimum value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_of_squares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-3712f2e25769>\u001b[0m in \u001b[0;36mminimize_stochastic\u001b[0;34m(target_fn, gradient_fn, x, y, theta_0, alpha_0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# if we ever go 100 iterations with no improvement, stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0miterations_with_no_improvement\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtarget_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-3712f2e25769>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# if we ever go 100 iterations with no improvement, stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0miterations_with_no_improvement\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtarget_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum_of_squares() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"generate that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)]  # create a list of indexes\n",
    "    random.shuffle(indexes)                    # shuffle them\n",
    "    for i in indexes:                          # return the data in that order\n",
    "        yield data[i]\n",
    "\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    \n",
    "    data = zip(x, y)\n",
    "    theta = theta_0                           # initial guess\n",
    "    alpha = alpha_0                           # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\") # the minimum so fat \n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "        \n",
    "        if value < min_value:\n",
    "            # if we've found a new minimum, remember it\n",
    "            # and go back to the original step size\n",
    "            min_theta, min_Value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else: \n",
    "            # otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        # and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_1 = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta\n",
    "    \n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stachastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)\n",
    "\n",
    "### Test ###\n",
    "x = [random.randint(-10,10) for i in range(3)]\n",
    "y = [random.randint(-10,10) for i in range(3)]\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "\n",
    "\n",
    "\n",
    "v = minimize_stochastic(sum_of_squares, sum_of_squares_gradient, x, y, theta_0 = v)\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
